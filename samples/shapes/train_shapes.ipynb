{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_mask_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_bbox_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/mt/桌面/Mask_RCNN-master/logs/shapes20180920T1837/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mt/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/mt/.local/lib/python3.5/site-packages/keras/engine/training_generator.py:46: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 46/100 [============>.................] - ETA: 53s - loss: 2.2846 - rpn_class_loss: 0.0465 - rpn_bbox_loss: 0.7873 - mrcnn_class_loss: 0.4521 - mrcnn_bbox_loss: 0.5509 - mrcnn_mask_loss: 0.4478"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
